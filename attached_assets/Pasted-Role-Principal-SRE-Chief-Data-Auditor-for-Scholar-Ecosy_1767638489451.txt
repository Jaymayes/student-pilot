Role: Principal SRE & Chief Data Auditor for Scholar Ecosystem
Mode: Max Autonomous
Initial State: Read‑Only/Diagnostic
Constraint: No production writes, schema changes, or config edits without HUMAN_APPROVAL_REQUIRED

Objective
Run a full, evidence‑based audit and RCA of the Scholar Ecosystem to eliminate false positives, reconcile conflicting reports, prove end‑to‑end operation across Marketing, Lead Gen, Revenue, Learning, and Telemetry, and prepare PRs for Issues A–D.

Context and Targets
Prove “live and fully autonomous with human‑in‑the‑loop.”
SLOs: 99.9% uptime; P95 ≤150ms for critical endpoints.
Conflicts to reconcile:
A2 /ready: conflicting 404 vs 200 reports; re‑verify contract parity.
A7 P95: reports from ~216–559 ms; SendGrid check allegedly adds ~330 ms.
A8: stale incident banners; dashboard shows $0 revenue due to demo/test filtering; “Revenue Blocked” banner likely operational (A3 orchestration not run) vs pipeline failure.
Snapshots show 8/8 healthy; require canonical verification.
2. Guardrails and Operating Rules

Environments: Use Staging for active probes. If writes are needed, use namespace=simulated_audit with idempotent cleanup.
Rate limits: Cap probes at ≤2 QPS per service; stagger tests to avoid load‑induced noise. Abort if any endpoint >5% error rate in a 60‑sec window.
Timebox: Default audit runtime ≤90 minutes; request HUMAN_APPROVAL_REQUIRED to extend.
Docs‑First: Prepare PRs to ECOSYSTEM_README.md/runbooks for any architecture/config deltas; do not merge.
Compliance hygiene: Do not capture live PII in artifacts; redact secrets; confirm FERPA/COPPA posture where applicable.
3. Execution Plan
Phase 1 – System Discovery and Canonical Truth

Enumerate all 8 services via repo(s), manifests, ECOSYSTEM_README; record commit SHAs, versions, deployment timestamps, and environments.
Build system_map.json: service, purpose, inbound/outbound APIs, auth, secrets used (names only), queues/topics, data stores, and A8 ingestion paths.
Probe /health and /ready across Staging and Prod; compute P50/P95/P99 over at least 200 requests per critical endpoint; include confidence intervals. Resolve:
A2 /ready: authoritative result and contract.
A7 latency: isolate components (network, app, outbound providers such as SendGrid); quantify each contribution.
Phase 2 – Security and Connectivity

Verify HTTPS/TLS, API‑key auth, and Secrets usage; prove no hard‑coded credentials.
Produce connectivity_matrix.csv: for each cross‑app call, show success %, auth failures, latency distribution, and retry counts.
Phase 3 – Resiliency and False‑Positive Triage

Extract timeouts, retry/backoff, and circuit‑breaker configs; validate breaker states under induced faults in Staging.
Reproduce prior alerts; classify Confirmed Issue vs False Positive with logs/traces/metrics. Include clock‑skew checks between services as a potential FP source.
Draft monitoring rule changes as PRs (thresholds, durations, dedupe) to reduce noisy alerts.
Phase 4 – End‑to‑End Functional Probes (Staging, namespace=simulated_audit)

Marketing/SEO (A7): sitemap expansion and attribution events; verify async ingestion path; ensure no synchronous third‑party calls on request path.
Lead Gen: capture → CRM/lead store; verify webhook or message bus delivery with idempotency keys.
Revenue:
B2C: Stripe in test mode; verify fee/markup calculations where applicable.
B2B: provider funnel (Lead→Demo→Contract→Live); verify 3% provider fee and 4x AI services markup; confirm Finance entries posted to the correct streams.
Learning/Student Value: document hub ingestion; essay/AI workflows; success metrics logging.
A8 Telemetry: confirm events from all domains land in A8 with correct schema, version, env, namespace, and app version tags; confirm visibility in tiles and raw stores. Explain “$0 revenue” if due to filter/demo mode.
Phase 5 – Performance, Data Quality, and RCA

Hotspots: P50/P95/P99 per critical endpoint; correlate with CPU/memory/queue depth; identify synchronous chokepoints and propose 202‑then‑worker refactors with quantified impact.
Data lineage: validate schemas/versions for A8; run round‑trip checks from emission to A8 tiles; verify no schema drift; include sample payloads.
RCA: 5‑Whys and a fault tree; explicitly address “Revenue Blocked” banner (operational mode vs fault), and whether A3 orchestration is required for revenue to appear.
4. Deliverables and Evidence
Create /reports/scholar_audit/YYYYMMDD-HHMM/ with:

system_map.json, slo_metrics.json, connectivity_matrix.csv
security_checklist.md, resiliency_config.md
latency_profiles.csv, e2e_results.json
a8_data_lineage.md, a8_validation_results.json
rca.md (executive summary + 5‑Whys + fault tree)
evidence/: raw logs, trace IDs, screenshots of A8 tiles (staging), and query commands
monitoring_rule_pr.md (alert tuning proposals) All simulated_audit data must carry TTL=14 days and a cleanup script.
5. PR Proposals (Drafts only; do not merge)

Issue A (A2): pr_proposals/issue_a_a2_ready_endpoint.md — implement /ready contract and align health/readiness semantics; add contract test.
Issue B (A7): pr_proposals/issue_b_a7_async_ingestion.md — convert blocking ops to async worker (202 Accepted + job), add idempotency keys, and circuit breaker on outbound providers; include expected P95 improvement.
Issue C (A8): pr_proposals/issue_c_a8_stale_banners.md — add TTL, auto‑clear on recovery, admin clear endpoint; include UX acceptance tests.
Issue D (A8): pr_proposals/issue_d_a8_demo_mode.md — “Demo Mode” toggle to render clearly labeled simulated/test revenue without polluting live analytics; include namespace filter and UI badge.
6. Start‑Now Steps (first three actions)

Action 1: Enumerate services and capture commit SHAs, versions, and endpoint maps; write system_map.json.
Action 2: Probe /health and /ready in Staging and Prod with capped QPS; compute P50/P95/P99; reconcile A2 and A7 conflicts with fresh evidence.
Action 3: Emit minimal events per domain to A8 Staging (namespace=simulated_audit); verify appearance in tiles and raw stores; attach evidence.
7. Stop Conditions and Human Gates

Pause and raise HUMAN_APPROVAL_REQUIRED if any production mutation is needed; if SLO breaches exceed 2x; if security misconfigurations could expose PII; or if schema drift may corrupt analytics.
After artifacts and PR drafts are produced, post a read‑only “Audit Status” panel to A8 Staging with links to artifacts, then STOP and await CEO approval.
8. Success Criteria

All core workflows pass Staging probes; P95 ≤150ms on critical endpoints or a quantified remediation plan exists; telemetry is visible and correct in A8; “Revenue Blocked” and “$0 revenue” are explained with evidence; false positives are eliminated or alert rules adjusted via PR.