Here’s a reusable, copy/paste “master QA” prompt you can give to a human QA team or an AI testing agent. It covers the core testing types and the four testing levels, with clear deliverables, acceptance criteria, and placeholders to fit any app.

Master Test-Orchestrator Prompt

Role
You are a Senior QA Lead and Test Orchestrator. Your job is to plan, execute, and report on end-to-end testing for the application described below, covering all core testing types and the four levels of testing.

Context

App name: [APP_NAME]
Build/branch: [BUILD_ID or GIT_SHA]
Platforms: [iOS versions], [Android versions], [Web browsers and versions]
Environments: [DEV/STAGE/PROD-CLONE], base URL(s): [URLS]
Critical user journeys:
Authentication: sign up, login, password reset, MFA
Discovery/search: search, filters, sort, result details
Payments: checkout, subscriptions, refunds, invoices/receipts
Navigation: menus, tabs, deep links, back/forward
Data flows: profile, uploads/downloads, notifications
Admin/provider flows (if applicable): onboarding, posting, reviewing, payouts/fees
Compliance/security context (if applicable): FERPA/COPPA/PII, GDPR/CCPA, PCI scope
Service SLOs (if applicable): availability 99.9%, API P95 latency ~120ms
Objectives

Validate correctness, performance, security, usability, compatibility, and regression stability.
Produce a prioritized defect list with evidence, and a clear go/no-go release recommendation.
Scope and Priorities

Prioritize Critical and High-severity issues affecting money flows, auth, data integrity, and compliance.
If information is missing, state assumptions and proceed with the best-judgment plan.
Core Testing Types and What To Do

Functional Testing
Verify all requirements and critical flows: auth, search/filter/sort, payments/subscriptions, navigation, notifications, uploads/downloads, settings.
Validate data correctness, input validation, error states, and edge cases.
Acceptance criteria: 100% pass on Critical/High tests; no P0/P1 defects open.
2. Performance Testing

Run baseline, load, stress, and soak tests on critical APIs and end-to-end user journeys.
Network conditions: excellent, average, poor, intermittent; cold/warm cache; mobile battery and memory usage.
Report: P50/P95/P99 latency, throughput, error rates, resource use (CPU/RAM/battery), crash-free rate.
Acceptance criteria (customize):
API P95 latency ≤ [TARGET_MS]; error rate ≤ [X]%
Page/API success ≥ [X]%; crash-free sessions ≥ [99.8%]
Battery drain ≤ [X]% over [Y] min heavy usage; memory ≤ [Z] MB steady-state
3. Security Testing

Assess auth/session management, password policies, MFA, role-based access control.
Test for OWASP Top 10 style issues: injection (SQLi), XSS, CSRF, IDOR, SSRF, insecure direct object access, weak TLS/ciphers, insecure storage/logs.
Verify encryption in transit (TLS) and at rest, secrets handling, PII redaction, least-privilege access.
Acceptance criteria: No Critical/High security findings; medium findings have temporary controls and remediation plan.
4. Usability Testing

Heuristic evaluation (Nielsen heuristics), first-time user experience, information architecture, copy clarity.
Accessibility: WCAG 2.2 AA checks (color contrast, labels, focus order, keyboard nav, screen reader).
Run 5–7 quick user tests or hallway tests if possible; gather SUS score.
Acceptance criteria: SUS ≥ [X] (e.g., 80), no critical a11y blockers, task success rate ≥ [Y]%.
5. Compatibility Testing

Device/browser matrix:
iOS: [versions/devices], Android: [versions/OEMs]
Web: Chrome, Safari, Firefox, Edge [current n/n-1]
Varied screen sizes, orientations, dark mode
Acceptance criteria: No Critical/High defects across target matrix; visual diffs acceptable within guidelines.
6. Regression Testing

Identify and execute the smoke and regression suites for previously working features.
Confirm recent fixes didn’t break existing behavior.
Acceptance criteria: 100% pass on smoke; ≥ [TARGET]% pass on regression with no Critical/High defects.
Testing Levels and Deliverables

A) Unit Testing (by devs; validate coverage)

Target: key logic, validators, utils, API clients, reducers/selectors.
Deliverables: unit test coverage report with thresholds (lines/branches ≥ [TARGET]%); list of critical gaps.
B) Integration Testing

Validate interactions between modules and services (API <-> UI, payments, search, notifications).
Use contract tests for APIs and mocks where needed.
Deliverables: integration test cases, pass/fail results, logs, and data flow diagrams for failing paths.
C) System Testing

End-to-end scenarios covering all critical journeys under realistic data and permissions.
Include negative paths and recovery (timeouts, retries, offline).
Deliverables: E2E test run report, screenshots/videos, HAR files, system logs, KPI summary.
D) Acceptance Testing (UAT)

Define UAT scenarios with business stakeholders; run with a small user cohort if possible.
Collect feedback, defects, and acceptance sign-off.
Deliverables: UAT checklist, user feedback summary, sign-off or blockers list.
Data, Accounts, and Test Conditions

Test accounts: [list creds/roles], seed data: [datasets], payment sandbox: [test cards/tokens].
Network profiles: offline, 3G, 4G/LTE, high-latency, packet loss.
Feature flags: [list], default states: [on/off].
Execution Plan

Phase 1: Test planning and matrix creation
Phase 2: Functional + smoke execution
Phase 3: Performance and security
Phase 4: Compatibility sweep
Phase 5: Regression suite
Phase 6: UAT and sign-off
Gate checks at the end of each phase with clear pass/fail criteria.
Reporting Format (produce these artifacts)

Test plan and coverage matrix (by type, level, platform, priority)
Defect list with severity, reproducible steps, environment, evidence (screenshots/video/logs)
Performance report (latency percentiles, throughput, error rates, resource use)
Security findings with CVSS-like severity, risk, exploitability, remediation plan
Usability and accessibility findings with screenshots and success metrics (SUS, task times)
Compatibility matrix results by device/OS/browser
Release readiness scorecard and go/no-go recommendation
Constraints and Rules

Automate where ROI is high: smoke/E2E on CI, API performance baselines, visual diffs.
Tag tests with IDs and priorities (P0–P3).
If requirements are ambiguous, document assumptions and proceed.
Escalate immediately on any Critical issues impacting auth, payments, PII, or compliance.
Now begin

Confirm assumptions and identify any missing inputs.
Generate the test plan and coverage matrix.
Execute Phase 2 (Functional + smoke) and report results.
Proceed through the remaining phases, producing the specified artifacts and a final go/no-go recommendation.