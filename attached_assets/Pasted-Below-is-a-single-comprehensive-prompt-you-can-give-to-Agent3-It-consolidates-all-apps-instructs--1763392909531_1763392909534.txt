Below is a single, comprehensive prompt you can give to Agent3. It consolidates all apps, instructs a full E2E frontend and backend test without making any changes to code or configuration, and mandates a highly detailed report. It clearly delineates responsibilities per app and requires every per‑app section to include the app name and its APP_BASE_URL.

Prompt for Agent3 (copy/paste below)

Role and constraints

You are Agent3, the autonomous QA/E2E test agent for Scholar AI Advisor.
Objective: Execute an end-to-end, read-only test of both frontend and backend across all applications listed below. Do not change code, infrastructure, or configurations. Avoid destructive data actions. If a workflow requires writing to production, only proceed if an explicit test/sandbox mode is available; otherwise, skip and document the gap.
Deliverable: A single consolidated report plus per‑app sections. Every per‑app section header must include the app name and its base URL in the format: APP_NAME — APP_BASE_URL.
App registry (use these exact base URLs)

scholar_auth — https://scholar-auth-jamarrlmayes.replit.app
scholarship_api — https://scholarship-api-jamarrlmayes.replit.app
scholarship_agent — https://scholarship-agent-jamarrlmayes.replit.app
scholarship_sage — https://scholarship-sage-jamarrlmayes.replit.app
student_pilot — https://student-pilot-jamarrlmayes.replit.app
provider_register — https://provider-register-jamarrlmayes.replit.app
auto_page_maker — https://auto-page-maker-jamarrlmayes.replit.app
auto_com_center — https://auto-com-center-jamarrlmayes.replit.app
Global test scope and rules

No code/config changes. Read-only by default. Prefer GET/HEAD/OPTIONS. If a POST/PUT/PATCH/DELETE is essential to validate a flow, perform it only if an explicit “dry_run”, “test”, or “sandbox” flag is supported and visibly prevents persistent changes. If no such flag exists, skip and document.
Test both frontend UI and backend HTTP APIs. Use multiple requests to characterize latency distribution and reliability.
SLO baselines to assess: Availability target 99.9%; P95 latency target ≈120 ms; Error rate target <1%.
Security and compliance: Validate standard security headers, CORS policies, authentication boundaries, and any hints of FERPA/COPPA alignment where relevant to student data.
Accessibility: Assess keyboard-only navigation, color contrast, focus states, form labels, and run an automated audit where possible. Target WCAG 2.1 AA.
SEO (apps with public pages): Validate robots.txt, sitemap.xml, structured data, meta tags, canonicalization, core web vitals directionally.
Observability: Note server response headers, request IDs/trace IDs, and any visible health endpoints.
Evidence: For every finding, capture reproducible steps, endpoint paths, parameters, sample requests/responses (redact secrets), console/network logs, and any measurable metrics (latency, status code counts).
Never send actual user communications to real recipients. If any messaging/email/SMS capability is encountered, use log-only or sandbox routes when available; otherwise skip and document.
Required outputs

Report index: List all eight apps using the format “APP_NAME — APP_BASE_URL”, each linking to its section.
Cross-app E2E flows summary: One section describing end-to-end user journeys and integration touchpoints tested.
Per-app sections (one per app). For each section, strictly use this structure:
Header: “APP_NAME — APP_BASE_URL”
Live status and uptime snapshot: Is the base URL reachable? TLS valid? Any /health or /status endpoints? Summarize availability during test window.
API discovery and documentation: Note existence of /docs, /openapi.json, /swagger, or other developer docs. If none, derive an endpoint inventory from observed traffic/UI.
Frontend review:
Key pages reachable, navigation, layout integrity.
Console errors/warnings, network errors, CORS issues.
Accessibility notes (keyboard nav, focus, alt text, labels, contrast).
Core Web Vitals directional observations (LCP/INP/CLS if measurable).
Backend/endpoint checks:
Endpoint inventory with methods tested.
For each endpoint probed, record: URL, method, typical status codes, content-type, sample response schema, cache headers, security headers, and latency stats (P50/P95 across ≥20 requests where feasible).
Rate limiting and error handling behaviors.
Authentication/authorization: Identify auth scheme (e.g., JWT/cookies). Validate protected vs. public routes. Do not exfiltrate secrets.
Data integrity and privacy: Confirm that personally identifiable or sensitive student/provider data is not exposed in public routes or logs.
SLO assessment: Compare measured P95 latency and error rates to targets; call out any gaps.
Issues found: Each with severity (Critical/Major/Minor), Steps to Reproduce, Expected vs. Actual, Evidence (request/response snippets, logs), and suspected cause.
Recommendations (non-code-change in nature if possible, e.g., configuration or operational suggestions). Do not implement changes—only propose.
Cross-app executive summary: Top 5 platform-wide risks, top 5 fast-impact fixes, and a brief view of how current performance aligns with the strategic SLOs.
Execution phases

Phase 1 — Discovery

For each base URL:
Verify TLS/HTTPS, resolve DNS, and load the root path.
Attempt: /health, /status, /metrics, /ready, /live if present.
Attempt docs: /docs, /swagger, /openapi.json.
Crawl one level to find key routes and static assets; note any dead links.
Record any environment hints (version strings in headers/footers, build hashes, server headers).
Establish a safe testing posture per app: identify any “dry_run”, “test”, or “sandbox” query/body parameters or routes. If none exist and a flow requires writes, plan to skip that write and document the gap.
Phase 2 — E2E flow validations (read-only wherever writes are not sandboxed)

Student journey:
Land on a public scholarship page generated by auto_page_maker.
Follow CTAs to sign-in/sign-up (scholar_auth) without actually creating accounts unless there is an explicit test flag; verify the auth UI, endpoints, and error handling on invalid credentials in read-only fashion.
Navigate or simulate data pulls in student_pilot; observe scholarship_api requests that fetch lists/details. Validate filters, pagination, and response schemas via GET.
Interact with scholarship_sage UI (if a demo/test mode exists). Validate guardrails and responsible AI disclaimers. If chat requires writes, skip sending; document.
Observe whether auto_com_center endpoints are invoked for notifications; do not send live messages. If “dry_run” exists, use it and capture payload structure.
Provider journey:
Visit provider_register; review onboarding UI and data-entry validation. Do not submit unless a sandbox path exists. If available, use test mode and confirm non-persistence.
Validate provider-facing pages and any read-only views.
Campaigns:
On scholarship_agent, look for a dashboard/endpoints showing scheduled campaigns, tasks, or logs. If there is a “dry_run” preview, exercise it and capture outputs. Do not send or schedule new live campaigns.
Phase 3 — Measurements and checks (apply to each app)

Latency: For each observable, safe endpoint, make ≥20 GET requests spaced over the session; record P50/P95. Target P95 ≈120 ms. Note geographic variability if any.
Reliability: Track status code distribution, transient failures, and any 5xx/4xx anomalies.
Security headers: Check Strict-Transport-Security, Content-Security-Policy, X-Content-Type-Options, X-Frame-Options, Referrer-Policy, Permissions-Policy, and CORS behavior.
Caching: Note Cache-Control, ETag, Last-Modified usage. Confirm idempotent routes are cacheable where appropriate.
Accessibility: Note keyboard trap issues, missing labels, focus outline visibility, alt text on images, and color contrast problems.
SEO (public pages: auto_page_maker and any other public UIs):
robots.txt and sitemap.xml present and valid.
Canonical tags, meta titles/descriptions, Open Graph tags.
Structured data for Scholarship entities (JSON-LD) if applicable.
Lighthouse-like observations on performance and indexability (directional, not lab scores).
Privacy/data handling: Ensure no PII leaks in GET query strings, client-side logs, or error messages.
Per‑app guidance and focal points

scholar_auth — https://scholar-auth-jamarrlmayes.replit.app
Agent3: Validate login page loads and error handling for invalid credentials in read-only mode.
Probe auth API endpoints documents if present (e.g., /docs or /openapi.json).
Validate security headers, CORS policy, cookie flags (Secure, HttpOnly, SameSite) or JWT handling patterns visible in the UI flow.
Check rate limiting behavior on repeated invalid login attempts (non-destructive).
Confirm password reset/refresh endpoints existence without triggering if no sandbox mode.
2. scholarship_api — https://scholarship-api-jamarrlmayes.replit.app

Agent3: Inventory public GET endpoints (e.g., scholarship listing, detail, filters, pagination).
Validate response schemas (id, title, deadlines, eligibility, provider link), status codes, and cache headers.
Measure latency (≥20 samples) and error handling for invalid query params.
Confirm no PII exposed in responses or error messages.
3. scholarship_agent — https://scholarship-agent-jamarrlmayes.replit.app

Agent3: Identify endpoints/UI for campaign schedules, dry-run previews, and logs.
If a “dry_run/preview” parameter exists, exercise it to list or simulate campaign content; do not send real messages.
Capture payload templates, variables, and guardrails (unsub links, provider compliance statements).
4. scholarship_sage — https://scholarship-sage-jamarrlmayes.replit.app

Agent3: Validate UI loads and present disclaimers about responsible AI and no academic dishonesty.
If a test/demo chat mode exists, initiate a harmless prompt that does not store data; otherwise, review only.
Assess latency of initial page load and any visible inference endpoints (read-only).
5. student_pilot — https://student-pilot-jamarrlmayes.replit.app

Agent3: Validate dashboard and public/read-only pages. Observe network calls to scholarship_api.
Test filters/sorting/pagination purely via GET. Confirm resilience to malformed query params.
Confirm that no sensitive student data is visible without authentication.
6. provider_register — https://provider-register-jamarrlmayes.replit.app

Agent3: Validate provider onboarding UI fields and client-side validations.
Do not submit unless a sandbox/dry_run exists; if present, verify that submissions are explicitly marked non-persistent.
Confirm any fee disclosures (3% platform fee) are visible in UI copy if applicable.
7. auto_page_maker — https://auto-page-maker-jamarrlmayes.replit.app

Agent3: Validate SEO surfaces: robots.txt, sitemap.xml, canonical tags, meta tags, Open Graph, and JSON-LD structured data for scholarship entities if present.
Sample multiple auto-generated pages; check internal linking, pagination, and duplicate/near-duplicate content issues.
Measure page load responsiveness and basic Core Web Vitals signals directionally.
8. auto_com_center — https://auto-com-center-jamarrlmayes.replit.app

Agent3: Identify messaging endpoints and UI; only use preview/dry_run/test endpoints if available.
Validate unsubscribe/opt-out mechanics in templates (preview only).
Confirm that PII is not logged in client console or GET query strings.
Cross-app E2E integrations to observe

From auto_page_maker to scholar_auth to student_pilot to scholarship_api to scholarship_sage: Follow UI/navigation and observe network calls. Do not create accounts or submit personal data unless a test flag clearly prevents persistence.
Provider flow: provider_register to scholarship_api (read-only observation). Confirm that provider documentation or links suggest how listings integrate.
Messaging: Any UI action that would call auto_com_center must be observed without sending real messages; capture the request shape in dry_run/preview if possible.
Campaigns: scholarship_agent dry-run views should reflect integration points to auto_com_center or other services without executing sends.
Metrics to capture per app (put in a small table or list in each section)

Availability during test window: reachable vs. errors.
Latency: P50 and P95 per key endpoint, sample size count.
Error rate: count and percentage across sampled requests.
Security headers presence matrix.
Accessibility quick-scan notes.
SEO signals (where applicable).
Issue taxonomy and reporting format

Severity:
Critical: Security/PII exposure, major outage, or broken core flows.
Major: Significant functional, reliability, or performance deviations from targets.
Minor: Cosmetic, copy, or low-risk performance issues.
For each issue: Title, Severity, Affected app(s), Steps to Reproduce, Expected vs. Actual, Evidence (URLs, request/response snippets, console/network logs), Suspected cause, Suggested fix (no implementation—report only).
Final executive summary

Provide:
Overall pass/fail per app against basic reachability and key read-only checks.
SLO assessment vs targets (availability, P95 latency, error rate).
Top 5 cross-platform risks and their business impact.
Top 5 fast-impact fixes with estimated effort/impact (qualitative).
Notable dependencies or missing sandbox features blocking full E2E validation.
Completion criteria

Consolidated report delivered with:
Report index listing all eight apps in the exact “APP_NAME — APP_BASE_URL” format.
A full per-app section following the required structure.
Cross-app E2E flows summary.
Clear callouts where write operations were skipped due to lack of sandbox/dry_run modes.
No changes made to code, configuration, or production data beyond read-only observation and dry-run previews where explicitly supported.