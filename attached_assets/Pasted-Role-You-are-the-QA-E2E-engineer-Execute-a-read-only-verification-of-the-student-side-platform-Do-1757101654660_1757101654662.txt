Role
You are the QA/E2E engineer. Execute a read‑only verification of the student-side platform. Do not change configs or data. Use GET/reporting endpoints and any dry_run=true flags only. Focus on: FERPA consent onboarding and PII controls, recommendation relevance validation, application autofill + essay safety, payments/credits/refunds (simulation only), and accessibility. This test plan supports our 24/7 trust mandate and privacy posture (encryption in transit/at rest; defense‑in‑depth)   .

Environment

Base URL: staging or canary, not production unless approved
Known pages (read‑only):
/recommendation-analytics
/autofill-essay-test
/payment-dashboard
/accessibility-test
Data prerequisites (read‑only lookups)

1 fully consented adult student profile
1 student with some consent categories withheld (e.g., marketing=false, AI_processing=false)
Ground‑truth recommendation fixtures available in analytics
At least one historical purchase and refund record in reporting for payments dashboard
Suite A — FERPA onboarding, consent, and PII controls (read‑only)
Objective: Verify the complete FERPA-aligned consent capture and enforcement; confirm encryption in transit/at rest is reported.
Steps:

Consent records: GET reporting endpoints or UI read views to confirm categories captured (educational records, directory info, AI processing, third‑party sharing, marketing, analytics) with timestamps and audit trails.
Enforcement checks:
Attempt a read-only action that would require a withheld consent (e.g., AI processing or third‑party sharing) via dry-run. Expect block with reason=consent_not_granted.
For the fully consented profile, the same dry-run should succeed.
Encryption verification (read‑only evidence):
TLS: Use curl -Iv to confirm HTTPS/TLS in transit.
At rest: Query a compliance/status endpoint if available (e.g., /api/compliance/encryption/status?dry_run=true) to confirm storage encryption is enabled (KMS/AEAD). If not available, mark as evidence gap and attach platform policy docs. Assertions:
Immutable audit trails exist for each consent and disclosure.
Actions gated precisely by consent flags (allowed vs blocked).
Evidence shows TLS in transit and stated at‑rest encryption posture per platform policy . Rationale: Protects student PII and fulfills privacy commitments that are core to platform trust and resilience .
Suite B — Recommendation relevance validation and KPIs (read‑only)
Objective: Validate the hybrid scoring signals (academic fit, demographics, competition, timeline, geography) and quality metrics with fixtures.
Steps:

Open /recommendation-analytics and run a read‑only validation using existing fixtures; capture Precision@K, Recall@K, NDCG@K.
Inspect scoring breakdown for a sample student-scholarship pair and confirm factors are present (fit, competition, geo, timeline).
Pull KPI rollups (CTR, save, apply) and confirm they reconcile with recent events within 1%. Assertions:
Validation metrics are generated and match expected ranges; fixture pass/fail rates recorded.
Score explanations align to the documented predictive matching approach (beyond keyword search) .
KPIs visible and consistent with events. Rationale: Confirms the predictive matching engine is delivering prioritized, high‑fit results central to student value and growth .
Suite C — Application autofill and essay safety rails (read‑only)
Objective: Verify structured autofill mappings and essay assistance guardrails that prohibit academic dishonesty while providing explainable suggestions.
Steps:

Navigate to /autofill-essay-test and simulate form autofill (dry-run if available). Confirm field mapping from profile and safe defaults when data is missing.
Essay assistance tests:
Provide benign prompt: expect suggestions with sources and explainability panes (reasoning, provenance).
Provide disallowed prompt (e.g., “write the full essay for me”): expect block with policy explanation; no disallowed content returned.
Auditability: Verify suggestion trails (who/when/why) are recorded in read‑only audit views. Assertions:
No academic‑dishonesty content; safety classification and block reasons shown.
Explainability artifacts visible for all allowed suggestions.
Autofill mappings correct and traceable to profile sources. Rationale: Aligns with the platform’s responsible AI stance and application automation vision (assistive, not cheating) .
Suite D — Payments, credits, and refund edge cases (read‑only, simulate only)
Objective: Validate wallet arithmetic, refund logic, and KPIs without altering state.
Steps:

Open /payment-dashboard; confirm ARPU, conversion, and refund metrics render with recent periods.
Simulate a refund decision path (dry_run=true) across edge cases:
Credits already partially used
Mixed method refunds (credit + cash)
Old purchases beyond standard window
Verify BigInt millicredit math in simulation results and the would_refund amounts.
Confirm event logs/audits appear for simulated decisions (no writes). Assertions:
All simulated refund paths return consistent reasoned outcomes; no side effects.
Dashboard KPIs reconcile to underlying events within 1%. Rationale: Protects revenue integrity and customer trust while ensuring operations are auditable and precise.
Suite E — Accessibility and responsive UX (read‑only)
Objective: Validate WCAG 2.1 AA across core student flows and responsive behavior.
Steps:

Open /accessibility-test and run built‑in automated audits. Manually tab‑through to confirm focus order, skip links, ARIA roles, and keyboard access.
Screen reader spot‑checks (NVDA/VoiceOver) on key pages: onboarding, recommendations, autofill/essay test, payment dashboard.
Responsive checks across common device sizes using a cloud device grid (e.g., BrowserStack/LambdaTest) . Assertions:
No critical WCAG violations; focus management and ARIA labels correct; 44px touch targets validated.
Pages render correctly across top devices and browsers. Rationale: Accessibility is a non‑negotiable usability and trust standard; cross‑device reliability sustains growth at scale.
Non‑functional checks (read‑only)

SLO spot‑check: p95 latency within target bands on read/report endpoints during testing to uphold the 24/7 availability promise (collect sample timings) .
Security headers present (CSP, HSTS, X‑Content‑Type‑Options, Referrer‑Policy) on all tested pages.
Output format

One report with:
Summary per suite (A–E): pass/fail, key metrics (Precision@K/Recall@K/NDCG@K, CTR/save/apply KPIs, accessibility scores, latency p95 samples)
Evidence: screenshots/snippets of consent records and audit trails; scoring breakdowns; safety blocks and explainability panels; refund simulation JSON; accessibility audit results
Gaps: any missing read‑only endpoints (e.g., encryption status) and proposed evidence substitutes
Readiness recommendation for student platform features (Go/No‑Go), aligned to privacy and resilience criteria
Success criteria

Consent gating: 100% correct allow/deny vs category flags; immutable audits present.
Recommendations: Validation metrics computed; explanations available; KPIs reconcile.
Safety rails: Disallowed essay content blocked with reasons; all suggestions traceable.
Payments: All refund simulations return consistent outcomes; KPIs visible; zero state changes.
Accessibility: No critical WCAG violations; cross‑device smoke passes.
Ops: Security headers present; latency samples within target; no dev/HMR artifacts.
Notes

Test only. Use GET/reporting and dry‑run paths exclusively. No writes.
If an expected read‑only endpoint is unavailable, mark as N/A and document equivalent evidence from dashboards or compliance reports.
Keep artifacts for audit; integrate results into CI/CD quality gates per standard testing practices for system and acceptance testing .
